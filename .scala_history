def k():
return 'hello'
def k():
return "j"
def k():
println("hello")
print
print("hello")
def j():
print("hello")
def j():unit={
print("hello")
}
def j():Unit={
print ("hello")
}
j()
n='hello'
n="hello"
var n='hello'
var n="hello"
n
n.
def pk(Int x, Int y):Int ={
def pk( x:Int, y:Int ):Int ={
x=y
}
def pk( var x:Int, var y:Int ):Int ={
def pk( x:Int, y:Int ):Int ={
var n=x
}
def pk( x:Int, y:Int ):Int ={
var n=x
n=y
}
def pk( x:Int, y:Int ):Int ={
var n=x
return n
}
k=pk(1,2)
var k=pk(1,2)
k
sys.refaddr(k)
Sys.refaddr(k)
say Sys.refaddr(k)
val p="hello"
val p="ഹലോ"
p
p.length
p.getBytes
p.getBytes.length
val p="ഹലോ"
val n="hello"
n.getByte
n.getBytes
val j="ஐ"
j.getBytes
print(j)
println(j)
print(j) + print(j)
print(j); print(j)
print(j); print(j);print("hello")
print(j)
print(j).getClass
p
p.getBytes(1)
p.getChars(1,2)
p.getChars(1,2,"he",2))
p.getChars(1,2,"he",2)
p.getChars(1,2,Array("h","a"),2)
p.getChars(1,2,Array["h","a"],2)
p.getChars(1,2,Array('h','a'),2)
p.getChars(1,1,Array('h','a'),2)
p.getChars(0,1,Array('h','a'),2)
p.getChars(0,3,Array('h','a'),2)
p.getChars(0,3,Array('h','a'),0)
println
println("hello","p")
def p(n:Int):Int={
n=2
return n
}
def p(var n:Int):Int={
1 to 10
print(1 to 10)
list(1 to 10)
(1 to 10).max
(1 to 10).toList
Any.
var p:Any="abc"
p.toString.toInt
var p:Any="123"
p.toString.toInt
var p=(n:Int, p:Int, r:Int)=> { n + p}
p(1,2)
p(1,2,3)
val p=(n:Int, p:Int, r:Int)=> { n + p}
p(,1,2,3)
p(1,2,3)
p=(n:Int,p:Int)=> {n-p}
var p=(n:Int, p:Int, r:Int)=> { n + p}
p=(n:Int,p:Int)=> {n-p}
p=(n:Int,p:Int,q:Int)=> {n-p}
exit
var p=(n:Int, p:Int, r:Int)=> { n + p}
p
p.toString
p.getClass
var n=2
var j=(p:Int, q:Int)=> (n=1)
j(2,100)
n
p=(1 to 100)
var p=(1 to 100)
p.iterator
p.iterator.next
p
var s=(1 to 100).toSet
def isEven(i:Int)=i%2==0
s.filter(x=> isEven(x))
isEven(100)
var p=(x)=> isEven(x)
s.filter(p)
p
 var p=(1 to 100)
p.iterator.next
p.iterator.next()
p
j=p.iterator
var j=p.iterator
j.next()
p.iterator.next()
p.iterator.next().next
p.iterator.next()
s.filter(x=> isEven(x))
s=(1 to 100)
var s=(1 to 100)
def isEven(x:Int)=x%2==0
s.filter(isEven)
var p=(1 to 2)
var k=p.toSet()
p
p.toSet().exists(1)
p.exists
p
var p=(1 to 10)
p.exists(2)
p.exists(2=>2)
p.exists
p.exists()
p=(1 to 100)
var p=(1 to 100)
p.filter
"hello".tail
p
names=List("hello","world")
var names=List("hello","world")
names.map(_.head.toUpper + _.tail.toLower)
names.map(_*.head.toUpper + _*.tail.toLower)
names.map(_.head.toUpper )
names.map(_.head.toUpper + _.tail.toLower)
'a'.
var p=List(1,2,3)
p.foreach{
println(_)
}
p.foreach{
println(_)
}
var p=List((1,2),(2,3))
p.foreach{
println(_)
}
p.foreach{
println(_.1)
println(_(1))
p=(1,2)
var p=tuple(1,2)
var p=set(1,2)
var p=Iterator(1,2)
_(1)
var p=List((1,2),(2,3))
p.foreach {
println(_)
}
var j=range(-10, 0, 1)
var j=Range(-10, 0, 1)
j.toSet
j.toList()
j.toList
p
p.foreach{
println(_,_)
}
p.foreach{
print(_,_)
}
print("hello","jack")
p.foreach{
print(_.0,_.1)
p[0]
p(0)
p.foreach{
print(_(0))
}
p.foreach{
print(_.(0))
p.foreach{
print(_.1)}
print(_1)}
p.foreach{
print(_._1)}
p=List((1,2),(2,3))
var p=List((1,2),(2,3))
p.foreach{println(_._1))
p.foreach{println(_._1)}
p.foreach{println(_)}
p.foreach{println(_(0))}
(1,2).1
(1,2)(0)
(1,2)[0]
(1,2)._1
p.foreach{println(_.getClass)}
p.foreach{println(_)}
p.foreach{n=_}
p.foreach{var n=_}
p.foreach{var n=_; }
p.foreach{var n:Int=_; }
p.foreach{var n:Int=_; print(n)}
p.foreach{var n:List(Int)=_; print(n)}
p.foreach{var n:Int=_._1; print(n)}
p.foreach{var n:Int=_.1; print(n)}
p.foreach{var n:List[Int] = _._1; print(n)}
p.foreach{var n:List[Int] = _; print(n)}
p.foreach{var n:List[Int] = _;}
p.foreach{println(_)}
p.foreach{println(_.dtypes)}
p.foreach{println(_.getClass)}
(1,2,3) + (4,5,6)
List(1,2,3) + List(4,5,6)
List(1,2,3).addThen
p.foreach{println(_.next())}
List(1, 2, 3) foreach { _ => println("Hi") }
List(1, 2, (3,4)) foreach { _ => println("Hi") }
List(1, 2, (3,4)) foreach { _ foreach { _ => println("Hi")} }
List((1), (2), (3,4)) foreach { _ foreach { _ => println("Hi")} }
List((1), (2), (3,4)) foreach { _.toList foreach { _ => println("Hi")} }
var p:Any=1
List(List(1), List(2), List(3,4)) foreach { _.toList foreach { _ => println("Hi")} }
List(List(1), List(2), List(3,4)) foreach { _ foreach { _ => println("Hi")} }
 List(1, 2, (3,4)) foreach {prinln(_)
}
 List(1, 2, (3,4)) foreach {println(_)
}
 List(1, 2, (3,4)) foreach {var j:Any = _; println(j)}
 List(1, 2, (3,4)) foreach {var j:List = _; println(j)}
 List(1, 2, (3,4)) foreach {var j:Any = _; println(j)}
 List(1, 2) foreach {var j:Any = _; println(j)}
 List(1, 2) foreach {var j:Any = _}
print(None)
print(Null)
print(Nothing)
val x = {println("test"); 10}
x
lazy val x = {println("test"); 10}
x
var p=(x=> 10)
var p=(x)=>10
var p=(x=>10)
var p=(x:Int=>10)
var p=(x:Int)=>10)
var p=(x:Int)=>10
List(1,2) zip List(3,4)
List(1,2) zip List(3,4) map {case(a,b) => (a+b)}
(1,2).getClass
List(1,2).getClass
List(1,2).sortBy(x=>x)
List(1,2).sortBy(x> x-1 => True)
List(1,2).sortBy(x => True)
List(1,2).sortBy(_>_)
List(1,2).sortWith(_>_)
List(1,2).sortWithList(1, 2) foreach {var j:Any = _}
List(1, 2) foreach {var j:Any = _}
List(1, 2) foreach {var j:Any = _; return Void}
List(1, 2) foreach {var j:Any = _; Void}
List(1, 2) foreach {var j:Any = _; }
List(1, 2) foreach {var j:Any = 0; }
List(1, 2) foreach {var j:Any = 0; println(j)}
List(1, 2) foreach {var j:Int = 0; println(j)}
List(1, 2) foreach {println(j)}
List(1, 2) foreach {println(_)}
List(1, 2) foreach {var j:Int=0;println(_)}
List(1, 2) foreach { _ => var p:Any =_; println(p)}
List(1, 2) foreach { _ => var p:Any =_}
List(1, 2) foreach { _ => var p:Any =2}
List(1, 2) foreach { _ => var p:Any =2;println(p)}
List(1, 2) foreach { _ => var p:Any =_;println(p)}
List(1, 2) foreach { _ => var p:Any =_._1;println(p)}
List(1, 2) foreach { _ => var p:Any =1;println(p)}
var j={var p:Any =1;println(p)}
List(1,2) foreach { _ => j}
lazy var j={var p:Any =1;println(p)}
var j={var p:Any =1}
List(1,2) foreach { _ => j}
var j={var p:Any =1; p}
List(1,2) foreach { _ => j}
lazy var j={var p:Any =1; p}
exit
:quit
import scala.io._
val n=Source.fromFile
val n=Source.fromFile("/etc/passwd",'utf-8')
val n=Source.fromFile("/etc/passwd","utf-8")
n.next()
n.LineIterator()
n.LineIterator
n
n.getLines()
n.getLines().foreach()
n.getLines().foreach{println(_)}
val n=Source.fromFile("/etc/passwd","utf-8")
n.foreach{println(_)}
val n=Source.fromFile("/etc/passwd","utf-8")
n.getLines().filter(x=> x.split(":")[4]=="/bin/bash")
n.getLines().filter(x=> x.split(":")(4)=="/bin/bash")
n.getLines().filter(x=> x.split(":")(4)=="/bin/bash").getLines().foreach{println(_)}
n.getLines().filter(x=> x.split(":")(4)=="/bin/bash").foreach{println(_)}
n.getLines()
n.getLines().filter(x=> True).foreach{println(_)}
n.getLines().filter(x=> trrue).foreach{println(_)}
n.getLines().filter(x=> true).foreach{println(_)}
val n=Source.fromFile("/etc/passwd","utf-8")
val j=n.getLines()
j.getLines().filter(x=> true).foreach{println(_)}
n.getLines().filter(x=> true).foreach{println(_)}
val n=Source.fromFile("/etc/passwd","utf-8")
n.getLines().filter(x=> x.split(":")(7)=="/bin/bash").foreach{println(_)}
val n=Source.fromFile("/etc/passwd","utf-8")
n.getLines().filter(x=> x.split(":")(6)=="/bin/bash").foreach{println(_)}
val n=Source.fromFile("/etc/passwd","utf-8")
import scala.io._
val n=Source.fromFile("/etc/passwd","utf-8")
n.getLines.foreach{println(_)}
val n=Source.fromFile("/etc/passwd","utf-8")
n.getLines().filter(x=> x.split(":")(6)=="/bin/bash").foreach{println(_)}
val n=Source.fromFile("/etc/passwd","utf-8")
n.getLines.take(10).toList
val p=Source.fromFile("/etc/passwd","utf-8")
p.getLines().toStream.take(10)
n=p.getLines().toStream.take(10)
val ln=p.getLines().toStream.take(10)
ln
ln foreach prinln
ln foreach println
ln.iterator.next
import scala.io._
:quit
exit
:quit
sc.master
dir(sc)
sc.ne
sc.version
var p=sc.textFiles("file://etc/passwd")
var p=sc.textFile("file://etc/passwd")
p.count()
var p=sc.textFile("file:///etc/passwd")
p.count()
var j=p.map(x=> x.split(":")).filter(x=>x(6)=="/bin/bash")
j.show
j.show()
j.collect()
var j=p.map(x=> x.split(":")).map(x=>x(1)+"hello").filter(x=>x(6)=="/bin/bash")
j.collect()
var j=p.map(x=> x.split(":")).map(x=>x(1)+"hello").filter(x=>x==x)
j.collect()
sc.parallelize(range( 1 to 10).toList, 2)
sc.parallelize(Range( 1 to 10).toList, 2)
range(1 to 10)
Range(1 to 10)
Range(10)
range(10)
range(1,10,2)
Range(1,10,2)
Range(1,10)
sc.parallelize(Range(1,10),2)
sc.parallelize(Range(1,10),2).getNumPartitions
sc.parallelize(Range(1,10),2).filter(x=> x!=3).map(x=> x**3).collect()
sc.parallelize(Range(1,10),2).filter(x=> x!=3).map(x=> x*3).collect()
sc.parallelize(Range(1,10),2).filter(x=> x!=3).map(x=> (x,x*3)).collect()
List(1,2).toString
var p=List(1,2).toString
p
p(0)
var j=List(1,2)
var k=n.toString
var k=j.toString
k[0]
k(0)
var k=j.toString()
k(0)
k.toList
lazy var p=sc.parallelize(List(1,2))
lazy val p=sc.parallelize(List(1,2))
p
p(o)
p(0)
p.collect()
var n=spark.read.json("/tmp/abc.json")
n.age.collect()
n.select("age").collect()
n.map(x=>x.age).collect()
n.map(x=>x."age").collect()
n.map(x=>x.age).collect()
spark.sql.Row
n.map(x=>x.getClass).collect()
n
import spark.sql.Row
import org.apache.spark.sql.Row
n.map(x=>x.getClass).collect()
n.map(x=>x.(0)).collect()
n.map(x=>x(0)).collect()
n.map(x=>x.Row.(0)).collect()
n.map(x=>Row(0)).collect()
n.map(x=>Row(0)).toRDD.collect()
n.map(x=>Row(0).toSeq).collect()
val myFile=sc.textFile("/hdfs-site.xml")
myFile.flatMap(x=>x.split(" "))
var j=myFile.flatMap(x=>x.split(" "))
j.first()
val myFile=sc.textFile("/hdfs-site.xml")
myFile.collect()
val myFile=sc.textFile("hdfs://hdfs-site.xml")
myFile.collect()
val myFile=sc.textFile("hdfs:///hdfs-site.xml")
myFile.collect()
val myFile=sc.textFile("hdfs://127.0.0.1:9001/hdfs-site.xml")
myFile.collect()
val n=myFile.flatMap(x=>x.split(" "))
n.first()
(1,2)(2)
var p=(1,2)
p(1)
p._1
p._2
p._0
val cnts=n.reduceByKey( total, current => total + current)
val cnts=n.reduceByKey( (total, current) => total + current)
n.take(2)
n
var indcnt=n.map(_,1)
var indcnt=n.map((_,1))
var cnt=indcnt.reduceByKey(_+_)
cnt.collect()
cnt.sortBy(_.2,false).first()
cnt.sortBy(_._2,false).first()
cnt.sortBy(_._2,false).collect()
cnt.sortBy(_._2,false).saveAsTextFile("hdfs://localhost:9001/cnt.txt")
n.getNumPartitions
sc
val myFile=sc.textFile("hdfs://127.0.0.1:9001/hdfs-site.xml")
val myFile2=sc.textFile("hdfs://127.0.0.1:9001/cnt.txt/part-00000")
val p=myFile.count + myFile2.count
val p={myFile.count;thread.sleep(10000) ; myFile2.count}
spark
val p={myFile.count;Thread.sleep(10000) ; myFile2.count}
val p=zip(myFile.count, myFile2.count}
val p=zip(myFile.count, myFile2.count)
val p=myFile.count.zip(myFile2.count)
val p=zip(myFile,myFile2)
val p=myFile.zip(myFile2)
p.count
print myFile.toDF.join(myFile2.toDF).count
myFile.toDF.schema
myFile2.toDF.schema
myFile.toDF.join(myFile2.toDF,value)
myFile.toDF.join(myFile2.toDF,"value")
myFile.toDF.join(myFile2.toDF,"value").count
val myFile=sc.textFile("hdfs://127.0.0.1:9001/hdfs-site.xml")
val myFile2=sc.textFile("hdfs://127.0.0.1:9001/cnt.txt/part-00000")
myFile.count
myFile.toDF.join(myFile2.toDF,"value").count
val n=spark.read.json("/root/Downloads/spark/examples/src/main/resources/people.json")
n
n.select("name").count()
n.select("name")
n.select("name").show
n.select($"name").show
n.filter("n">100).show
n.filter($"n">100).show
n.filter($"age">100).show
n.filter("age").show
n.select("age").show
n.filter("age">30).show
n.filter($"age">30).show
n.filter($"age">=30).show
n.filter("name">="a").show
myFile.toDF("age","names")
myFile.show()
myFile.collect()
myFile2.collect()
myFile2.split(",").collect()
myFile2.map(x=>x.split(",")).collect()
val myDF2=myFile2.map(x=>x.split(",")).toDF("word","cnt")
myFile2.first(2)
myFile2.first(\)
myFile2.first()
val myDF2=myFile2.map(x=>x.split(",")).toDF("word")
myDF2.select("word(0)").show
myDF2.select($"word(0)").show
myDF2.select("word").show
myDF2.select("word"(0)).show
myDF2.select("word[0]").show
myDF2.select("word(0)").show
myDF2.select($"word(0)").show
myDF2.schema
myDF2.printschema
myDF2.printSchema
myDF2.select("word".getItem(0)).show
myDF2.select($"word".getItem(0)).show
spark.createDataFrame(myFile2.map(x=>Row(x),StructType(Array(StructField("name",StringType(),True))))
)
spark.createDataFrame(myFile2.map(x=>Row(x)),StructType(Array(StructField("name",StringType(),True)))
)
spark.createDataFrame(myFile2.map(x=>Row(x)),StructType(Array(StructField("name",StringType(),true))))
import org.apache.spark.sql._
spark.createDataFrame(myFile2.map(x=>Row(x)),StructType(Array(StructField("name",StringType(),true))))
spark.createDataFrame(myFile2.map(x=>Row(x)),StructType(Array(StructField("name",stringType(),true))))
import org.apache.spark.sql.StructField
import org.apache.spark.sql.types._
spark.createDataFrame(myFile2.map(x=>Row(x)),StructType(Array(StructField("name",stringType(),true))))
spark.createDataFrame(myFile2.map(x=>Row(x)),StructType(Array(StructField("name",StringType(),true))))
spark.createDataFrame(myFile2.map(x=>Row(x)),StructType(Array(StructField("name",StringType,true))))
spark.createDataFrame(myFile2.map(x=>Row(x)),StructType(Array(StructField("name",StringType,true)))).show()
spark.createDataFrame(myFile2.map(x=>Row(x)),StructType(Array(StructField("name",Array(String),true)))).show()
spark.createDataFrame(myFile2.map(x=>Row(x)),StructType(Array(StructField("name",Array[String],true)))).show()
myDF2.schema
spark.createDataFrame(myFile2.map(x=>Row(x)),StructType(Array(StructField("name",ArrayType(String),true)))).show()
spark.createDataFrame(myFile2.map(x=>Row(x)),StructType(Array(StructField("name",ArrayType[String],true)))).show()
spark.createDataFrame(myFile2.map(x=>Row(x)),StructType(Array(StructField("name",ArrayType[StringType],true)))).show()
spark.createDataFrame(myFile2.map(x=>Row(x)),StructType(Array(StructField("name",ArrayType(StringType),true)))).show()
spark.createDataFrame(myFile2.map(x=>Row(x)),StructType(Array(StructField("name",ArrayType[StringType,Boolean],true)))).show()
spark.createDataFrame(myFile2.map(x=>Row(x)),StructType(Array(StructField("name",ArrayType(StringType,Boolean),true)))).show()
spark.createDataFrame(myFile2.map(x=>Row(x)),StructType(Array(StructField("name",ArrayType(StringType,BooleanType),true)))).show()
spark.createDataFrame(myFile2.map(x=>Row(x)),StructType(Array(StructField("name",ArrayType(StringType,Boolean()),true)))).show()
spark.createDataFrame(myFile2.map(x=>Row(x)),StructType(Array(StructField("name",ArrayType(StringType,true),true)))).show()
spark.createDataFrame(myFile2.map(x=>Row(x)),StructType(Array(StructField("name",ArrayType[StringType,true],true)))).show()
spark.createDataFrame(myFile2.map(x=>Row(x)),StructType(Array(StructField("name",ArrayType[StringType,classOf[true]],true)))).show()
myDF2.schema
myDF2.printschema
myDF2.printSchema
spark.createDataFrame(myFile2.map(x=>Row(x)),StructType(Array(StructField("name",ArrayType(StringType,classOf(true)),true)))).show()
spark.createDataFrame(myFile2.map(x=>Row(x)),StructType(Array(StructField("name",ArrayType(StringType,true),true)))).show()
spark.createDataFrame(myFile2.map(x=>Row(x)),StructType(Array(StructField("name",ArrayType(String,true),true)))).show()
spark.createDataFrame(myFile2.map(x=>Row(x)),StructType(Array(StructField("name",ArrayType("String",true),true)))).show()
spark.createDataFrame(myFile2.map(x=>Row(x)),StructType(Array(StructField("name",ArrayType("StringType",true),true)))).show()
spark.createDataFrame(myFile2.map(x=>Row(x)),StructType(Array(StructField("name",ArrayType(StringType,true),true)))).show()
spark.createDataFrame(myFile2.split(",").map(x=>Row(x)),StructType(Array(StructField("name",ArrayType("StringType",true),true)))).show()
spark.createDataFrame(myFile2.split(",").map(x=>Row(x)),StructType(Array(StructField("name",ArrayType(StringType,true),true)))).show()
spark.createDataFrame(myFile2.map(x=>split(",")).map(x=>Row(x)),StructType(Array(StructField("name",ArrayType(StringType,true),true)))).show()
spark.createDataFrame(myFile2.map(x=>x.split(",")).map(x=>Row(x)),StructType(Array(StructField("name",ArrayType(StringType,true),true)))).show()
spark.createDataFrame(myFile2.map(x=>x.split(",")).map(x=>Row(x)),StructType(Array(StructField("name",ArrayType(StringType,true),true)))).select($"name".getItem(0)).show()
"hello"[1]
"hello"(1)
"hello"(1:4)
"hello".substring(1,2)
"hello".substring(1,3)
spark.createDataFrame(sc.textFile("file:///etc/passwd").map(x=>x.split(",").map(x=>Row(x)),StructType(Array(StructField("col1",StringType,true),
StructType(Array(StructField("col2",StringType,true),
StructType(Array(StructField("col3",StringType,true),
StructType(Array(StructField("col4",StringType,true),
StructType(Array(StructField("col5",StringType,true),
StructType(Array(StructField("col6",StringType,true),
StructType(Array(StructField("col7",StringType,true))))
)
val nwdf=spark.createDataFrame(sc.textFile("file:///etc/passwd").map(x=>x.split(",")).map(x=>Row(x)),StructType(Array(StructField("col1",StringType,true),
StructType(Array(StructField("col2",StringType,true),
StructType(Array(StructField("col3",StringType,true),
StructType(Array(StructField("col4",StringType,true),
StructType(Array(StructField("col5",StringType,true),
StructType(Array(StructField("col6",StringType,true),
StructType(Array(StructField("col7",StringType,true) )))
)
val nwdf=spark.createDataFrame(sc.textFile("file:///etc/passwd").map(x=>x.split(",")).map(x=>Row(x)),StructType(Array(StructField("col1",StringType,true),
StructField("col2",StringType,true),
StructField("col3",StringType,true),
StructField("col4",StringType,true),
StructField("col5",StringType,true),
StructField("col6",StringType,true),
StructField("col7",StringType,true) )))
nwdf.show
val nwdf=spark.createDataFrame(sc.textFile("file:///etc/passwd").map(x=>x.split(",")).map(x=>Row.fromSeq(x.toSeq)),StructType(Array(StructField("col1",StringType,true),
StructField("col2",StringType,true),
StructField("col3",StringType,true),
StructField("col4",StringType,true),
StructField("col5",StringType,true),
StructField("col6",StringType,true),
StructField("col7",StringType,true) )))
nwdf.show
val nwdf=spark.createDataFrame(sc.textFile("file:///etc/passwd").map(x=>x.split(":")).map(x=>Row.fromSeq(x.toSeq)),StructType(Array(StructField("col1",StringType,true),
StructField("col2",StringType,true),
StructField("col3",StringType,true),
StructField("col4",StringType,true),
StructField("col5",StringType,true),
StructField("col6",StringType,true),
StructField("col7",StringType,true) )))
nwdf.show
nwdf.groupBy("col6").show
nwdf.groupBy("col6")
val p=nwdf.groupBy("col6")
p
p.schema
p.count(col7)
p.agg(count(col7))
p.agg(count($"col7"))
p.agg(count($"col7")).show
val p=nwdf.groupBy("col6")
nwdf
val p=nwdf.groupBy("col6")
p.agg("col7").show
p.agg("col7").show()
p.agg($"col7").show()
val p=nwdf.groupBy("col6")
p.agg($"col7").show()
p.agg("col7").show()
val p=nwdf.groupBy("col6")
p.agg("col7").show
p.agg(count("col7")).show
p
p.toDebugString
p.rdd.toDebugString
p.toDF.rdd.toDebugString
p.toDF().rdd.toDebugString
val pq=spark.read.load("/root/Downloads/spark/examples/src/main/resources/users.parquet")
pq.show()
package pk1
case class intExtend(n:Int){
  def plus(p:Int)=n+p
  
}
object jk{
implicit def inttointExtend(n:Int):intExtend={
  intExtend(n)
}
case class stringExtend(n:Int){
  def plus(p:Int)="helo"
  
}
object pk{
implicit def strtointExtend(n:Int):stringExtend={
  stringExtend(n)
}
case class intExtend(n:Int){
  def plus(p:Int)=n+p
  
}
object jk{
implicit def inttointExtend(n:Int):intExtend={
  intExtend(n)
}
case class stringExtend(n:Int){
  def plus(p:Int)="helo"
  
}
object pk{
implicit def strtointExtend(n:Int):stringExtend={
  stringExtend(n)
}
import scala.language.implicitConversions
case class intExtend(n:Int){
  def plus(p:Int)=n+p
  
}
object jk{
implicit def inttointExtend(n:Int):intExtend={
  intExtend(n)
}
case class stringExtend(n:Int){
  def plus(p:Int)="helo"
  
}
object pk{
implicit def strtointExtend(n:Int):stringExtend={
  stringExtend(n)
}
println(1.plus(1))
val mysqlDf=spark.read.format("jdbc").option("url","jdbc:mysql://localhost/hell").option("driver","com.mysql.jdbc.Driver").options("dbtable","hell1").option("user","root").option("password","redhat").load
val mysqlDf=spark.read.format("jdbc").option("url","jdbc:mysql://localhost/hell").option("driver","com.mysql.jdbc.Driver").options("dbtable","hell1").option("user","root").option("password","redhat").load()
val mysqlDf=spark.read.format("jdbc").option("url","jdbc:mysql://localhost/hell").option("driver","com.mysql.jdbc.Driver").option("dbtable","hell1").option("user","root").option("password","redhat").load()
mysqlDf.printSchema
mysqlDf.first(10)
mysqlDf.rdd.first(10)
mysqlDf.rdd.first()
mysqlDf.first()
mysqlDf.show()
val n=mysqlDf.select($"name")
n.write.format("jdbc").option("url","jdbc:mysql://localhost/hell").option("driver","com.mysql.jdbc.Driver").option("dbtable","hell2").option("user","root").option("password","redhat")
n.show
n.write.format("jdbc").option("url","jdbc:mysql://localhost/hell").option("driver","com.mysql.jdbc.Driver").option("dbtable","hell2").option("user","root").option("password","redhat")
n.show
val p=n.write.format("jdbc").option("url","jdbc:mysql://localhost/hell").option("driver","com.mysql.jdbc.Driver").option("dbtable","hell2").option("user","root").option("password","redhat")
p
p.save
val p=n.write.format("jdbc").option("url","jdbc:mysql://localhost/hell").option("driver","com.mysql.jdbc.Driver").option("dbtable","hell2").option("user","root").option("password","redhat").option("SaveMode","append")
p.save
val p=n.write.format("jdbc").option("url","jdbc:mysql://localhost/hell").option("driver","com.mysql.jdbc.Driver").option("dbtable","hell2").option("user","root").option("password","redhat").option("SaveMode.append","append")
p.save
val p=n.write.format("jdbc").option("url","jdbc:mysql://localhost/hell").option("driver","com.mysql.jdbc.Driver").option("dbtable","hell2").option("user","root").option("password","redhat").mode("append")
p.save
val p=n.write.format("jdbc").option("url","jdbc:mysql://localhost/hell").option("driver","com.mysql.jdbc.Driver").option("dbtable","hell2").option("user","root").option("password","redhat").mode(SaveMode.Append)
val p=n.write.format("jdbc").option("url","jdbc:mysql://localhost/hell").option("driver","com.mysql.jdbc.Driver").option("dbtable","hell2").option("user","root").option("password","redhat").mode(org.apache.spark.sql.SaveMode.Append)
p.save
val up:String = (n:String)=>_.toUpper
val up:String = (n:String)=> _.toUpper()
val up:String = (n:String)=> n.toUpper()
val up:String = (n:String)=> _.toUpperCase
val up:String = (n:String)=> n.toUpperCase
val up:String => (n:String)= n.toUpperCase
val up:String = udf((n:String)=> n.toUpperCase)
val up = udf((n:String)=> n.toUpperCase)
up("asf")
case class Person(name:String)
val df =  Seq(Person("asfsd")).toDF
df.select(up("name")).show90
df.select(up("name")).show()
df.select(up($"name")).show()
val up1:String => (String)=> n.toUpperCase
val up1:String => (String)= n.toUpperCase
val up1:String = (String)=> n.toUpperCase
val up1:String = (String)=> _.toUpperCase
val up1:String = String=> _.toUpperCase
val up1:String = (n:String)=> _.toUpperCase
val up1:String = (n:String)=> n.toUpperCase
val up1:String = (n:String)= n.toUpperCase
val up1 = (n:String)=> n.toUpperCase
val up1 = (n:String):String => n.toUpperCase
n
val up1 = (hello:String):String => hello.toUpperCase
val up1:String = (hello:String) => hello.toUpperCase
val up1 = (hello:String) => hello.toUpperCase
val up1 = (hello:String):String => hello.toUpperCase
val up1 = {(hello:String) => hello.toUpperCase}:String
val up1 = (hello:String) => hello.toUpperCase
import org.apache.sql.udf
import org.apache.sql.functions.udf
import org.apache.spark.functions.udf
import org.apache.spark.sql.functions.udf
val udfup1=udf(up1)
n.show()
n.select(udfup1($"name")).show
val upper:String => String = _.toUpperCase
upper
val upper:String => (n:String) = _.toUpperCase
val p=(n:String)=> n+"hello"
p
val p=(n:String):String => n+"hello"
val p=(n:String) => n+"hello"
val upper:(String,Int) => String = _.toUpperCase
val upper:(String,Int) => String = _._1.toUpperCase
val upper:(Int) => String = _.toString.toUpperCase
upper(1)
sc
sc.textFile("/root/Downloads/NYPD_7_Major_Felony_Incidents.csv")
sc.textFile("/root/Downloads/NYPD_7_Major_Felony_Incidents.csv").first()
val p=sc.textFile("/root/Downloads/NYPD_7_Major_Felony_Incidents.csv")
data.first()
p.first()
head=p.first()
val head=p.first()
val dt=p.tail()
val dt=p.filter(x=>x==head)
dt.count()
val dt=p.filter(x=>x!=head)
dt.count()
head.count()
head
fields=header.split(",")
val fields=header.split(",")
val fields=head.split(",")
val fields=head.split(",").map(x=>x.replace(" ","_"))
val schm=fields.foreach{
var n=""
varl schm=fields.foreach{n=n+x;println(n)}
var schm=fields.foreach{n=n+x;println(n)}
var schm=fields.foreach{n=n+x}
var schm=fields.foreach{x=> n=n+x}
schm
n
var schm=fields.foreach{x=> n=n+"StructType(\"" + x + "StringType,true),"}
n
n=""
var schm=fields.foreach{x=> n=n+"StructType(\"" + x + "StringType,true),"}
schm
n
n=""
var schm=fields.foreach{x=> n=n+"StructType(\"" + x + ",\"StringType,true),"}
n
n=""
var schm=fields.foreach{x=> n=n+"StructType(\"" + x + "\",StringType,true),"}
n
println(n)
var schm=StructType(fields.map(StructField(_,StringType,true)))
import org.apache.spark.sql.types
var schm=StructType(fields.map(StructField(_,StringType,true)))
import org.apache.spark.sql.types._
var schm=StructType(fields.map(StructField(_,StringType,true)))
schm
var p=spark.createDataFrame(dt,schm)
var p=spark.createDataFrame(dt.map(Row(_)),schm)
dt
dt.first()
var p=spark.createDataFrame(dt.map(_.split(",")),schm)
var p=spark.createDataFrame(dt.map(_.split(",")).map(Row(_)),schm)
dt.first()
head
var p=spark.createDataFrame(dt.map(_.split(",")).map(x=>Row(_)),schm)
var p=spark.createDataFrame(dt.map(_.split(",")).map(x=>Row.fromSeq(_.toSeq)),schm)
import org.apache.spark.sql.Row
var p=spark.createDataFrame(dt.map(_.split(",")).map(x=>Row.fromSeq(_.toSeq)),schm)
var p=spark.createDataFrame(dt.map(_.split(",")).map(x=>Row.fromSeq(x.toSeq)),schm)
p.first
p.select("Location 1").show
head
fields
p.select("Location_1").show
p.select("XCordinate").show
fields
p.select("XCoordinate").show
p.first
fields
p.withColumn("name",lit(0))
p.withColumn("name",lit(0)).first
p.withColumn(("name","why"),(lit(0),lit(1))).first
fields
dt.first()
var p=spark.createDataFrame(dt.map(_.split(",")).map(x=>Row.fromSeq((x.take(19) :+ x(19) :+ x(20)).toSeq)),schm)
p.first()
dt.map(_.split(",")).map(x=>Row.fromSeq((x.take(19) :+ x(19) :+ x(20)).toSeq)).first()
var p=spark.createDataFrame(dt.map(_.split(",")).map(x=>Row.fromSeq((x.take(19) :+ x(19) + x(20)).toSeq)),schm)
p.columns
p.first()
p.select("Location_1").first()
